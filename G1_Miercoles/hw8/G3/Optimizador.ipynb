{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "850999c6",
      "metadata": {
        "id": "850999c6"
      },
      "source": [
        "# Optimizador Genético Básico sin librerías\n",
        "Algoritmos de optimización sin usar librerías externas: fuerza bruta, Monte Carlo y gradiente ascendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ced69e64",
      "metadata": {
        "id": "ced69e64"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Función objetivo a optimizar\n",
        "def f8(x, y):\n",
        "    return math.sin(x) * math.cos(y) + math.exp(-(x**2 + y**2)/10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36fb5ae3",
      "metadata": {
        "id": "36fb5ae3"
      },
      "source": [
        "\n",
        "### Clase del Optimizador Genético Básico\n",
        "La clase `OptimizadorGeneticoBasico` contiene tres métodos de optimización: fuerza bruta, Monte Carlo y gradiente ascendente.\n",
        "\n",
        "A continuación, implementaremos estos tres métodos de optimización dentro de la clase. Todos se utilizan para encontrar el máximo de la función objetivo **f8**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a139227f",
      "metadata": {
        "id": "a139227f"
      },
      "outputs": [],
      "source": [
        "# Clase con los tres métodos de optimización\n",
        "class OptimizadorGeneticoBasico:\n",
        "    def __init__(self, funcion):\n",
        "        self.func = funcion\n",
        "\n",
        "    # Fuerza Bruta\n",
        "    def fuerza_bruta(self, x_min, x_max, y_min, y_max, paso=1.0):\n",
        "        max_val = float('-inf')\n",
        "        opt_x = opt_y = None\n",
        "\n",
        "        xi = x_min\n",
        "        while xi <= x_max:\n",
        "            yi = y_min\n",
        "            while yi <= y_max:\n",
        "                val = self.func(xi, yi)\n",
        "                if val > max_val:\n",
        "                    max_val = val\n",
        "                    opt_x = xi\n",
        "                    opt_y = yi\n",
        "                yi += paso\n",
        "            xi += paso\n",
        "\n",
        "        return opt_x, opt_y, max_val\n",
        "\n",
        "    # Monte Carlo\n",
        "    def monte_carlo(self, x_min, x_max, y_min, y_max, muestras=1000):\n",
        "        max_val = float('-inf')\n",
        "        opt_x = opt_y = None\n",
        "\n",
        "        for _ in range(muestras):\n",
        "            xi = x_min + (x_max - x_min) * self.rand()\n",
        "            yi = y_min + (y_max - y_min) * self.rand()\n",
        "            val = self.func(xi, yi)\n",
        "            if val > max_val:\n",
        "                max_val = val\n",
        "                opt_x = xi\n",
        "                opt_y = yi\n",
        "\n",
        "        return opt_x, opt_y, max_val\n",
        "\n",
        "    # Generador de número pseudoaleatorio simple\n",
        "    def rand(self):\n",
        "        if not hasattr(self, 'seed'):\n",
        "            self.seed = 123456789\n",
        "        self.seed = (1103515245 * self.seed + 12345) % (2**31)\n",
        "        return self.seed / (2**31)\n",
        "\n",
        "    # Gradiente Ascendente\n",
        "    def gradiente_ascendente(self, x_init, y_init, tasa=0.01, iteraciones=1000):\n",
        "        x = x_init\n",
        "        y = y_init\n",
        "        delta = 1e-5\n",
        "\n",
        "        for _ in range(iteraciones):\n",
        "            df_dx = (self.func(x + delta, y) - self.func(x, y)) / delta\n",
        "            df_dy = (self.func(x, y + delta) - self.func(x, y)) / delta\n",
        "\n",
        "            x += tasa * df_dx\n",
        "            y += tasa * df_dy\n",
        "\n",
        "        val = self.func(x, y)\n",
        "        return x, y, val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9bf7b86",
      "metadata": {
        "id": "b9bf7b86"
      },
      "source": [
        "\n",
        "### Crear Instancia del Optimizador y Parámetros\n",
        "Ahora que tenemos la clase definida, creamos una instancia del optimizador y configuramos los parámetros que vamos a utilizar. El rango de búsqueda de **x** y **y** es de **-5 a 5** para ambos.\n",
        "\n",
        "**Nota:** Se pueden cambiar estos valores según las necesidades del problema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "73878e4d",
      "metadata": {
        "id": "73878e4d"
      },
      "outputs": [],
      "source": [
        "# Crear instancia del optimizador\n",
        "opt = OptimizadorGeneticoBasico(f8)\n",
        "\n",
        "# Definir parámetros del problema\n",
        "x_min, x_max = -5, 5\n",
        "y_min, y_max = -5, 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62383bb0",
      "metadata": {
        "id": "62383bb0"
      },
      "source": [
        "\n",
        "### Optimización por Fuerza Bruta\n",
        "En la **optimización por fuerza bruta**, probamos todas las combinaciones posibles de **x** y **y** dentro de un rango determinado. Calculamos el valor de la función en cada punto y elegimos el que nos da el valor máximo.\n",
        "\n",
        "Este es un método sencillo pero lento para encontrar el máximo de la función.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d2ab6429",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ab6429",
        "outputId": "483e40b2-a352-4c5f-86d3-9ba7a37284f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fuerza Bruta:\n",
            "x = 1.5, y = 0.0, valor máximo = 1.7960112053634316\n"
          ]
        }
      ],
      "source": [
        "# Optimización por Fuerza Bruta\n",
        "fb_x, fb_y, fb_val = opt.fuerza_bruta(x_min, x_max, y_min, y_max, paso=0.5)\n",
        "print(\" Fuerza Bruta:\")\n",
        "print(f\"x = {fb_x}, y = {fb_y}, valor máximo = {fb_val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6228fb1d",
      "metadata": {
        "id": "6228fb1d"
      },
      "source": [
        "\n",
        "### Optimización por Monte Carlo\n",
        "El **método de Monte Carlo** es un enfoque probabilístico. En lugar de recorrer todos los puntos posibles, generamos puntos aleatorios en el espacio de búsqueda y calculamos el valor de la función en esos puntos.\n",
        "\n",
        "Este método es más rápido que la fuerza bruta, pero depende del número de muestras generadas. Cuantas más muestras, más preciso será el resultado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "426cc084",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "426cc084",
        "outputId": "08203b4a-1ba2-4465-a2b9-0022145da960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Monte Carlo:\n",
            "x = 1.1854714341461658, y = 0.09236505720764399, valor máximo = 1.7908792373191385\n"
          ]
        }
      ],
      "source": [
        "# Optimización por Monte Carlo\n",
        "mc_x, mc_y, mc_val = opt.monte_carlo(x_min, x_max, y_min, y_max, muestras=1000)\n",
        "print(\"\\nMonte Carlo:\")\n",
        "print(f\"x = {mc_x}, y = {mc_y}, valor máximo = {mc_val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69c868c",
      "metadata": {
        "id": "d69c868c"
      },
      "source": [
        "\n",
        "###  Optimización por Gradiente Ascendente\n",
        "El **gradiente ascendente** es un método matemático que se basa en el cálculo de derivadas. Empieza en un punto inicial y ajusta las variables **x** y **y** en función de la pendiente de la función en ese punto.\n",
        "\n",
        "Este método es muy rápido, pero es sensible al punto inicial y a la tasa de aprendizaje. Si la tasa es muy alta, puede no converger, y si es demasiado baja, puede ser muy lento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0fe0724d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fe0724d",
        "outputId": "5181f626-0d38-437b-ac1c-9cec5bf0f605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Gradiente Ascendente:\n",
            "x = 1.3444377437344937, y = -4.999989201337485e-06, valor máximo = 1.8091330203775233\n"
          ]
        }
      ],
      "source": [
        "# Optimización por Gradiente Ascendente\n",
        "ga_x, ga_y, ga_val = opt.gradiente_ascendente(x_init=1, y_init=1, tasa=0.05, iteraciones=500)\n",
        "print(\"\\n Gradiente Ascendente:\")\n",
        "print(f\"x = {ga_x}, y = {ga_y}, valor máximo = {ga_val}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}